{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Lglw-LkAKOle"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XGpK6EopYBLq"
   },
   "outputs": [],
   "source": [
    "cuda0 = torch.device('cuda:0')\n",
    "cuda1 = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "69X_-qPkKOlh"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "global epoch\n",
    "epoch = 0\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "\n",
    "log = logging.getLogger('werkzeug')\n",
    "log.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BExJk30EKOli"
   },
   "outputs": [],
   "source": [
    "def get_next_state(board, player, action):\n",
    "    nextBoard = np.copy(board)\n",
    "    lign = 0\n",
    "    while nextBoard[lign][action] != 0 and sum(\n",
    "            (x != 0) for x in nextBoard.transpose().flatten()) < 42:\n",
    "        lign = (lign + 1) % 6\n",
    "    nextBoard[lign][action] = player\n",
    "\n",
    "    # Return the new game, but\n",
    "    # change the perspective of the game with negative\n",
    "    return nextBoard, -player\n",
    "\n",
    "\n",
    "def has_legal_moves(board):\n",
    "    for colonne in range(7):\n",
    "        if not colonnepleine(board, colonne) and sum(\n",
    "                (x != 0) for x in board.transpose().flatten()) < 42:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_valid_moves(board):\n",
    "    # All moves are invalid by default\n",
    "    valid_moves = [0] * 7\n",
    "\n",
    "    for colonne in range(7):\n",
    "        if not colonnepleine(board, colonne) and sum(\n",
    "                (x != 0) for x in board.transpose().flatten()) < 42:\n",
    "            valid_moves[colonne] = 1\n",
    "\n",
    "    return valid_moves\n",
    "\n",
    "\n",
    "def get_reward_for_player(board, player):\n",
    "    # return None if not ended, 1 if player 1 wins, -1 if player 1 lost\n",
    "\n",
    "    if colWin(board) or ligneWin(board) or diagRigthToLeftWin(board) or diagLeftToRightWin(board):\n",
    "        if winner == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    # if has_legal_moves(board):\n",
    "    #     return None\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_canonical_board(board, player):\n",
    "    return player * board\n",
    "\n",
    "\n",
    "def ucb_score(parent, child):\n",
    "    \"\"\"\n",
    "    The score for an action that would transition between the parent and child.\n",
    "    \"\"\"\n",
    "    prior_score = child.prior * math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "    if (child.visit_count > 0) and (child.prior > 0):\n",
    "        # The value of the child is from the perspective of the opposing player\n",
    "        value_score = -child.value()\n",
    "    else:\n",
    "        value_score = 0\n",
    "\n",
    "    return value_score + prior_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "i5IcroHfKOlk"
   },
   "outputs": [],
   "source": [
    "def add_dirichlet_noise(child_priors):\n",
    "        colonnes = torch.sort(child_priors, descending=True)\n",
    "        action2rotation = [0, 1, 2, 3, 4, 5, 6]\n",
    "        action_idxs = action2rotation[colonnes.indices[0][0]]\n",
    "        valid_child_priors = child_priors[0][action_idxs]  # select only legal moves entries in child_priors array\n",
    "        valid_child_priors = 0.75 * valid_child_priors.cpu().detach().numpy() + 0.25 * np.random.dirichlet(\n",
    "            np.zeros([len(child_priors[0])], dtype=np.float32) + 192)\n",
    "        child_priors = torch.tensor([valid_child_priors], dtype=torch.float32)\n",
    "        return (child_priors)\n",
    "def convert_grid(grid, gridnorme):\n",
    "    for i in range(6):\n",
    "        for j in range(7):\n",
    "            if (grid[i][j] == 1):\n",
    "                gridnorme[i][j] = -1\n",
    "            elif (grid[i][j] == 2):\n",
    "                gridnorme[i][j] = 1\n",
    "            else:\n",
    "                gridnorme[i][j] =0\n",
    "\n",
    "\n",
    "def convert_zero(grid, gridnorme):\n",
    "    for i in range(6):\n",
    "        for j in range(7):\n",
    "            if (grid[i][j] == 0):\n",
    "                gridnorme[i][j] = 1\n",
    "            else:\n",
    "                gridnorme[i][j] =0\n",
    "def convert_one(grid, gridnorme):\n",
    "    for i in range(6):\n",
    "        for j in range(7):\n",
    "            if (grid[i][j] == 1):\n",
    "                gridnorme[i][j] = 1\n",
    "            else:\n",
    "                gridnorme[i][j] = 0\n",
    "def convert_neg_one(grid, gridnorme):\n",
    "    for i in range(6):\n",
    "        for j in range(7):\n",
    "            if (grid[i][j] == -1):\n",
    "                gridnorme[i][j] = 1\n",
    "            else:\n",
    "                gridnorme[i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-61Hi3drKOlk"
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, prior, to_play):\n",
    "        self.visit_count = 0\n",
    "        self.to_play = to_play\n",
    "        self.prior = prior\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.state = None\n",
    "\n",
    "    def expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self):\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visit_count\n",
    "\n",
    "    def select_child(self):\n",
    "        \"\"\"\n",
    "        Select the child with the highest UCB score.\n",
    "        \"\"\"\n",
    "        best_score = -np.inf\n",
    "        best_action = -1\n",
    "        best_child = None\n",
    "\n",
    "        for action, child in self.children.items():\n",
    "            score = ucb_score(self, child)\n",
    "            if not colonnepleine(self.state, action):\n",
    "                if score> best_score:\n",
    "                    best_score = score\n",
    "                    best_action = action\n",
    "                    best_child = child\n",
    "\n",
    "        return best_action, best_child\n",
    "\n",
    "    def expand(self, state, to_play, action_probs):\n",
    "        \"\"\"\n",
    "        We expand a node and keep track of the prior policy probability given by neural network\n",
    "        \"\"\"\n",
    "        self.to_play = to_play\n",
    "        self.state = state\n",
    "        for i in range(len(action_probs[0])):\n",
    "          if action_probs[0][i].data in torch.topk(action_probs[0], 7).values:\n",
    "            self.children[i] = Node(prior=action_probs[0][i].item(), to_play=self.to_play * -1)\n",
    "          else:\n",
    "            self.children[i] = Node(prior=0, to_play=self.to_play * -1)\n",
    "\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Debugger pretty print node info\n",
    "        \"\"\"\n",
    "        prior = \"{0:.2f}\".format(self.prior)\n",
    "        return \"{} Prior: {} Count: {} Value: {}\".format(self.state.__str__(), prior, self.visit_count, self.value())\n",
    "                                                         \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class ConvBlock(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvBlock, self).__init__()\n",
    "#         self.action_size = 7\n",
    "#         self.conv1 = nn.Conv2d(3, 512, kernel_size=4, stride=1, padding=1).cuda()\n",
    "#         self.bn1 = nn.BatchNorm2d(512).cuda()\n",
    "#         self.conv2 = nn.Conv2d(512, 1024, kernel_size=4, stride=1, padding=1).cuda()\n",
    "#         self.bn2 = nn.BatchNorm2d(1024).cuda()\n",
    "#         self.conv3 = nn.Conv2d(1024, 2048, kernel_size=4, stride=1, padding=1).cuda()\n",
    "#         self.bn3 = nn.BatchNorm2d(2048).cuda()\n",
    "#         self.conv4 = nn.Conv2d(2048, 1024, kernel_size=4, stride=1, padding=1).cuda()\n",
    "#         self.bn4 = nn.BatchNorm2d(1024).cuda()\n",
    "#         self.conv5 = nn.Conv2d(1024, 512, kernel_size=4, stride=1, padding=1).cuda()\n",
    "#         self.bn5 = nn.BatchNorm2d(512).cuda()\n",
    "\n",
    "\n",
    "#     def forward(self, s):\n",
    "#         s = s.view(-1, 3, 6, 7)  # batch_size x channels x board_x x board_y\n",
    "#         s = F.relu(self.bn1(self.conv1(s)))\n",
    "#         s = F.relu(self.bn2(self.conv2(s)))\n",
    "#         s = F.relu(self.bn3(self.conv3(s)))\n",
    "#         s = F.relu(self.bn4(self.conv4(s)))\n",
    "#         s = F.relu(self.bn5(self.conv5(s)))\n",
    "\n",
    "#         return s\n",
    "\n",
    "\n",
    "# class ResBlock(nn.Module):\n",
    "#     def __init__(self, inplanes=512, planes=512, stride=1, downsample=None):\n",
    "#         super(ResBlock, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n",
    "#                                padding=1, bias=False).cuda()\n",
    "#         self.bn1 = nn.BatchNorm2d(planes).cuda()\n",
    "#         self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "#                                padding=1, bias=False).cuda()\n",
    "#         self.bn2 = nn.BatchNorm2d(planes).cuda()\n",
    "#         self.drp = nn.Dropout(0.3).cuda()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         residual = x\n",
    "#         out = self.conv1(x)\n",
    "#         out = F.relu(self.bn1(out))\n",
    "#         out = self.drp(out)\n",
    "#         out = self.conv2(out)\n",
    "#         out = F.relu(self.bn2(out))\n",
    "#         out = self.drp(out)\n",
    "#         out += residual\n",
    "#         out = F.relu(out)\n",
    "#         return out\n",
    "\n",
    "\n",
    "# class OutBlock(nn.Module):\n",
    "#     # shape=6*7*32\n",
    "#     shape = 1024\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(OutBlock, self).__init__()\n",
    "\n",
    "#         self.fc1 = nn.Linear(self.shape, 512)\n",
    "#         self.fc2 = nn.Linear(512, 1)\n",
    "#         self.drp = nn.Dropout(0.3)\n",
    "\n",
    "\n",
    "\n",
    "#         self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "#         self.fc = nn.Linear(512, 7)\n",
    "\n",
    "#     def forward(self, s):\n",
    "#         v = s.view(-1, self.shape)  # batch_size X channel X height X width\n",
    "#         v = self.drp(F.relu(self.fc1(v)))\n",
    "#         v = torch.tanh(self.fc2(v))\n",
    "\n",
    "\n",
    "#         p = s.view(-1, self.shape)\n",
    "#         p = self.drp(F.relu(self.fc1(p)))\n",
    "#         p = self.fc(p)\n",
    "#         p = self.logsoftmax(p).exp()\n",
    "#         return p, v\n",
    "\n",
    "\n",
    "# class ConnectNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConnectNet, self).__init__()\n",
    "#         self.conv = ConvBlock().cuda()\n",
    "#         for block in range(20):\n",
    "#             setattr(self, \"res_%i\" % block, ResBlock().cuda())\n",
    "#         self.outblock = OutBlock()\n",
    "\n",
    "#     def forward(self, s):\n",
    "#         s = self.conv(s)\n",
    "#         for block in range(20):\n",
    "#             s = getattr(self, \"res_%i\" % block)(s)\n",
    "#         s = self.outblock(s)\n",
    "#         return s\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         \"\"\"\n",
    "#         Initialize weights of layers using Kaiming Normal (He et al.) as argument of \"Apply\" function of\n",
    "#         \"nn.Module\"\n",
    "#         :param m: Layer to initialize\n",
    "#         :return: None\n",
    "#         \"\"\"\n",
    "#         if isinstance(self.conv.conv1, nn.Conv2d):\n",
    "#             torch.nn.init.xavier_uniform_(self.conv.conv1.weight)\n",
    "#             torch.nn.init.zeros_(self.conv.conv1.bias)\n",
    "#         if isinstance(self.conv.bn1, nn.BatchNorm2d):\n",
    "#             torch.nn.init.normal_(self.conv.bn1.weight.data, mean=1, std=0.02)\n",
    "#             torch.nn.init.constant_(self.conv.bn1.bias.data, 0)\n",
    "#         if isinstance(self.conv.conv2, nn.Conv2d):\n",
    "#             torch.nn.init.xavier_uniform_(self.conv.conv2.weight)\n",
    "#             torch.nn.init.zeros_(self.conv.conv2.bias)\n",
    "#         if isinstance(self.conv.bn2, nn.BatchNorm2d):\n",
    "#             torch.nn.init.normal_(self.conv.bn2.weight.data, mean=1, std=0.02)\n",
    "#             torch.nn.init.constant_(self.conv.bn2.bias.data, 0)\n",
    "#         if isinstance(self.conv.conv3, nn.Conv2d):\n",
    "#             torch.nn.init.xavier_uniform_(self.conv.conv3.weight)\n",
    "#             torch.nn.init.zeros_(self.conv.conv3.bias)\n",
    "#         if isinstance(self.conv.bn3, nn.BatchNorm2d):\n",
    "#             torch.nn.init.normal_(self.conv.bn3.weight.data, mean=1, std=0.02)\n",
    "#             torch.nn.init.constant_(self.conv.bn3.bias.data, 0)\n",
    "#         if isinstance(self.conv.conv4, nn.Conv2d):\n",
    "#             torch.nn.init.xavier_uniform_(self.conv.conv4.weight)\n",
    "#             torch.nn.init.zeros_(self.conv.conv4.bias)\n",
    "#         if isinstance(self.conv.bn4, nn.BatchNorm2d):\n",
    "#             torch.nn.init.normal_(self.conv.bn4.weight.data, mean=1, std=0.02)\n",
    "#             torch.nn.init.constant_(self.conv.bn4.bias.data, 0)\n",
    "#         if isinstance(self.conv.conv5, nn.Conv2d):\n",
    "#             torch.nn.init.xavier_uniform_(self.conv.conv5.weight)\n",
    "#             torch.nn.init.zeros_(self.conv.conv5.bias)\n",
    "#         if isinstance(self.conv.bn5, nn.BatchNorm2d):\n",
    "#             torch.nn.init.normal_(self.conv.bn5.weight.data, mean=1, std=0.02)\n",
    "#             torch.nn.init.constant_(self.conv.bn5.bias.data, 0)\n",
    "\n",
    "#     # Making the body\n",
    "\n",
    "#     # Making the body\n",
    "\n",
    "\n",
    "\n",
    "# Making the body\n",
    "def colonnepleine(state, colonne):\n",
    "    grid = state.copy()\n",
    "    grid = grid.transpose()\n",
    "    plein = sum((x == 0) for x in grid[colonne]) == 0\n",
    "    return plein\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.action_size = 7\n",
    "        self.conv1 = nn.Conv2d(3, 1024, kernel_size=4, stride=1, padding=1).cuda()\n",
    "        self.bn1 = nn.BatchNorm2d(1024).cuda()\n",
    "        self.conv2 = nn.Conv2d(1024, 2048, kernel_size=4, stride=1, padding=1).cuda()\n",
    "        self.bn2 = nn.BatchNorm2d(2048).cuda()\n",
    "        self.conv3 = nn.Conv2d(2048, 4096, kernel_size=4, stride=1, padding=1).cuda()\n",
    "        self.bn3 = nn.BatchNorm2d(4096).cuda()\n",
    "        self.conv4 = nn.Conv2d(4096, 2048, kernel_size=4, stride=1, padding=1).cuda()\n",
    "        self.bn4 = nn.BatchNorm2d(2048).cuda()\n",
    "        self.conv5 = nn.Conv2d(2048, 512, kernel_size=4, stride=1, padding=1).cuda()\n",
    "        self.bn5 = nn.BatchNorm2d(512).cuda()\n",
    "\n",
    "\n",
    "    def forward(self, s):\n",
    "        s = s.view(-1, 3, 6, 7)  # batch_size x channels x board_x x board_y\n",
    "        s = F.relu(self.bn1(self.conv1(s)))\n",
    "        s = F.relu(self.bn2(self.conv2(s)))\n",
    "        s = F.relu(self.bn3(self.conv3(s)))\n",
    "        s = F.relu(self.bn4(self.conv4(s)))\n",
    "        s = F.relu(self.bn5(self.conv5(s)))\n",
    "\n",
    "        return s\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, inplanes=512, planes=512, stride=1, downsample=None):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False).cuda()\n",
    "        self.bn1 = nn.BatchNorm2d(planes).cuda()\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False).cuda()\n",
    "        self.bn2 = nn.BatchNorm2d(planes).cuda()\n",
    "        self.drp = nn.Dropout(0.3).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(self.bn1(out))\n",
    "        out = self.drp(out)\n",
    "        out = self.conv2(out)\n",
    "        out = F.relu(self.bn2(out))\n",
    "        out = self.drp(out)\n",
    "        out += residual\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class OutBlock(nn.Module):\n",
    "    # shape=6*7*32\n",
    "    shape = 1024\n",
    "\n",
    "    def __init__(self):\n",
    "        super(OutBlock, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(self.shape, 512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "        self.drp = nn.Dropout(0.3)\n",
    "\n",
    "\n",
    "\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        self.fc = nn.Linear(512, 7)\n",
    "\n",
    "    def forward(self, s):\n",
    "        v = s.view(-1, self.shape)  # batch_size X channel X height X width\n",
    "        v = self.drp(F.relu(self.fc1(v)))\n",
    "        v = torch.tanh(self.fc2(v))\n",
    "\n",
    "\n",
    "        p = s.view(-1, self.shape)\n",
    "        p = self.drp(F.relu(self.fc1(p)))\n",
    "        p = self.fc(p)\n",
    "        p = self.logsoftmax(p).exp()\n",
    "        return p, v\n",
    "\n",
    "\n",
    "class ConnectNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConnectNet, self).__init__()\n",
    "        self.conv = ConvBlock().cuda()\n",
    "        for block in range(30):\n",
    "            setattr(self, \"res_%i\" % block, ResBlock().cuda())\n",
    "        self.outblock = OutBlock()\n",
    "\n",
    "    def forward(self, s):\n",
    "        s = self.conv(s)\n",
    "        for block in range(30):\n",
    "            s = getattr(self, \"res_%i\" % block)(s)\n",
    "        s = self.outblock(s)\n",
    "        return s\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights of layers using Kaiming Normal (He et al.) as argument of \"Apply\" function of\n",
    "        \"nn.Module\"\n",
    "        :param m: Layer to initialize\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if isinstance(self.conv.conv1, nn.Conv2d):\n",
    "            torch.nn.init.xavier_uniform_(self.conv.conv1.weight)\n",
    "            torch.nn.init.zeros_(self.conv.conv1.bias)\n",
    "        if isinstance(self.conv.bn1, nn.BatchNorm2d):\n",
    "            torch.nn.init.normal_(self.conv.bn1.weight.data, mean=1, std=0.02)\n",
    "            torch.nn.init.constant_(self.conv.bn1.bias.data, 0)\n",
    "        if isinstance(self.conv.conv2, nn.Conv2d):\n",
    "            torch.nn.init.xavier_uniform_(self.conv.conv2.weight)\n",
    "            torch.nn.init.zeros_(self.conv.conv2.bias)\n",
    "        if isinstance(self.conv.bn2, nn.BatchNorm2d):\n",
    "            torch.nn.init.normal_(self.conv.bn2.weight.data, mean=1, std=0.02)\n",
    "            torch.nn.init.constant_(self.conv.bn2.bias.data, 0)\n",
    "        if isinstance(self.conv.conv3, nn.Conv2d):\n",
    "            torch.nn.init.xavier_uniform_(self.conv.conv3.weight)\n",
    "            torch.nn.init.zeros_(self.conv.conv3.bias)\n",
    "        if isinstance(self.conv.bn3, nn.BatchNorm2d):\n",
    "            torch.nn.init.normal_(self.conv.bn3.weight.data, mean=1, std=0.02)\n",
    "            torch.nn.init.constant_(self.conv.bn3.bias.data, 0)\n",
    "        if isinstance(self.conv.conv4, nn.Conv2d):\n",
    "            torch.nn.init.xavier_uniform_(self.conv.conv4.weight)\n",
    "            torch.nn.init.zeros_(self.conv.conv4.bias)\n",
    "        if isinstance(self.conv.bn4, nn.BatchNorm2d):\n",
    "            torch.nn.init.normal_(self.conv.bn4.weight.data, mean=1, std=0.02)\n",
    "            torch.nn.init.constant_(self.conv.bn4.bias.data, 0)\n",
    "        if isinstance(self.conv.conv5, nn.Conv2d):\n",
    "            torch.nn.init.xavier_uniform_(self.conv.conv5.weight)\n",
    "            torch.nn.init.zeros_(self.conv.conv5.bias)\n",
    "        if isinstance(self.conv.bn5, nn.BatchNorm2d):\n",
    "            torch.nn.init.normal_(self.conv.bn5.weight.data, mean=1, std=0.02)\n",
    "            torch.nn.init.constant_(self.conv.bn5.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VjlWf12-KOlt"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cnn = ConnectNet()\n",
    "cnn.init_weights()\n",
    "cnn = nn.DataParallel(cnn)\n",
    "cnn.to(device)\n",
    "\n",
    "cnn_iter1 = ConnectNet().to(cuda0)\n",
    "cnn_iter1.init_weights()\n",
    "n_step = 0\n",
    "\n",
    "Step = namedtuple('Step', ['state', 'action', 'reward'])\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Step:\n",
    "    state: []\n",
    "    action: []\n",
    "    reward: int = 0\n",
    "\n",
    "\n",
    "global memory\n",
    "\n",
    "global rewards\n",
    "rewards = []\n",
    "\n",
    "lossreward = nn.MSELoss()\n",
    "# lossreward = nn.BCEWithLogitsLoss()\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.SGD(cnn.parameters(),lr=0.01,momentum=0.9,weight_decay=5e-4)\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.0000001)\n",
    "reward = 0.0\n",
    "global gridnorme\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def treeSearch(batch):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    values = []\n",
    "    for series in batch:\n",
    "        inputs.append(series.state)\n",
    "        targets.append(series.action)\n",
    "        values.append(series.reward)\n",
    "    return torch.tensor([t.numpy() for t in inputs], dtype=torch.float), torch.tensor([t.numpy() for t in targets],\n",
    "                                                                                      dtype=torch.float), torch.tensor(\n",
    "        [t.numpy() for t in values], dtype=torch.float)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sample_batch(batch_size):  # creates an iterator that returns random batches\n",
    "    ofs = 0\n",
    "    vals = list(memory)\n",
    "    np.random.shuffle(vals)\n",
    "    while (ofs + 1) * batch_size <= len(memory):\n",
    "        yield vals[ofs * batch_size:(ofs + 1) * batch_size]\n",
    "        ofs += 1\n",
    "\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "\n",
    "def contains(subseq, inseq):\n",
    "    return any(inseq[pos:pos + len(subseq)] == subseq for pos in range(0, len(inseq) - len(subseq) + 1))\n",
    "\n",
    "\n",
    "winner = 0\n",
    "\n",
    "\n",
    "def ligneWin(gridnorme):\n",
    "    global winner\n",
    "    expected = False\n",
    "    for lign in gridnorme:\n",
    "        if contains([1, 1, 1, 1], lign.tolist()):\n",
    "            winner = 1\n",
    "        if contains([-1, -1, -1, -1], lign.tolist()):\n",
    "            winner = -1\n",
    "        expected = expected or contains([1, 1, 1, 1], lign.tolist()) or contains([-1, -1, -1, -1], lign.tolist())\n",
    "    return expected\n",
    "\n",
    "\n",
    "def colWin(gridnorme):\n",
    "    return ligneWin(gridnorme.transpose())\n",
    "\n",
    "\n",
    "def diagLeftToRightWin(gridnorme):\n",
    "    griddiag = np.zeros(shape=(7, 6))\n",
    "    griddiag[0] = [gridnorme[0][3], gridnorme[1][4], gridnorme[2][5], gridnorme[3][6], 0, 0]\n",
    "    griddiag[1] = [gridnorme[0][2], gridnorme[1][3], gridnorme[2][4], gridnorme[3][5], gridnorme[4][6], 0]\n",
    "    griddiag[2] = [gridnorme[0][1], gridnorme[1][2], gridnorme[2][3], gridnorme[3][4], gridnorme[4][5], gridnorme[5][6]]\n",
    "    griddiag[3] = [gridnorme[0][0], gridnorme[1][1], gridnorme[2][2], gridnorme[3][3], gridnorme[4][4], gridnorme[5][5]]\n",
    "    griddiag[4] = [gridnorme[1][0], gridnorme[2][1], gridnorme[3][2], gridnorme[4][3], gridnorme[5][4], 0]\n",
    "    griddiag[5] = [gridnorme[2][0], gridnorme[2][1], gridnorme[2][2], gridnorme[2][3], 0, 0]\n",
    "    return ligneWin(griddiag)\n",
    "\n",
    "\n",
    "def diagRigthToLeftWin(gridnorme):\n",
    "    griddiag = np.zeros(shape=(7, 6))\n",
    "    griddiag[0] = [gridnorme[3][0], gridnorme[2][1], gridnorme[1][2], gridnorme[0][3], 0, 0]\n",
    "    griddiag[1] = [gridnorme[4][0], gridnorme[3][0], gridnorme[2][1], gridnorme[1][2], gridnorme[0][3], 0]\n",
    "    griddiag[2] = [gridnorme[5][0], gridnorme[4][0], gridnorme[3][0], gridnorme[2][1], gridnorme[1][2], gridnorme[0][3]]\n",
    "    griddiag[3] = [gridnorme[5][1], gridnorme[4][2], gridnorme[3][3], gridnorme[2][4], gridnorme[1][5], gridnorme[0][6]]\n",
    "    griddiag[4] = [gridnorme[4][1], gridnorme[3][2], gridnorme[2][3], gridnorme[1][4], gridnorme[0][5], 0]\n",
    "    griddiag[5] = [gridnorme[3][1], gridnorme[2][2], gridnorme[1][3], gridnorme[0][4], 0, 0]\n",
    "\n",
    "    return ligneWin(griddiag)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "memory = deque()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "maxWin = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TqenSZ-hKOlw"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "def loadbrain2():\n",
    "    global cnn, optimizer, rewardcnn, cnnred, rewardcnnred\n",
    "    if os.path.isfile('bestrandom200.pth'):\n",
    "        print(\"=> loading checkpoint... \")\n",
    "       \n",
    "        checkpoint = torch.load('bestrandom200.pth', map_location=cuda0)\n",
    "        cnn_iter1.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        \n",
    "       \n",
    "\n",
    "        print(\"done !\")\n",
    "    else:\n",
    "        print(\"no checkpoint found...\")\n",
    "\n",
    "\n",
    "def loadbrain1():\n",
    "    global cnn, optimizer, rewardcnn, cnnred, rewardcnnred\n",
    "    if os.path.isfile('bestrandom.pth'):\n",
    "        print(\"=> loading checkpoint... \")\n",
    "        checkpoint = torch.load('bestrandom.pth', map_location=cuda0)\n",
    "        cnn.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "      \n",
    "        \n",
    "       \n",
    "\n",
    "        print(\"done !\")\n",
    "    else:\n",
    "        print(\"no checkpoint found...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3YqafRVhUMnF",
    "outputId": "35b66fe9-a438-4d42-e8ad-455dbf46ca8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint... \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DataParallel:\n\tMissing key(s) in state_dict: \"module.conv.conv1.weight\", \"module.conv.conv1.bias\", \"module.conv.bn1.weight\", \"module.conv.bn1.bias\", \"module.conv.bn1.running_mean\", \"module.conv.bn1.running_var\", \"module.conv.conv2.weight\", \"module.conv.conv2.bias\", \"module.conv.bn2.weight\", \"module.conv.bn2.bias\", \"module.conv.bn2.running_mean\", \"module.conv.bn2.running_var\", \"module.conv.conv3.weight\", \"module.conv.conv3.bias\", \"module.conv.bn3.weight\", \"module.conv.bn3.bias\", \"module.conv.bn3.running_mean\", \"module.conv.bn3.running_var\", \"module.conv.conv4.weight\", \"module.conv.conv4.bias\", \"module.conv.bn4.weight\", \"module.conv.bn4.bias\", \"module.conv.bn4.running_mean\", \"module.conv.bn4.running_var\", \"module.conv.conv5.weight\", \"module.conv.conv5.bias\", \"module.conv.bn5.weight\", \"module.conv.bn5.bias\", \"module.conv.bn5.running_mean\", \"module.conv.bn5.running_var\", \"module.res_0.conv1.weight\", \"module.res_0.bn1.weight\", \"module.res_0.bn1.bias\", \"module.res_0.bn1.running_mean\", \"module.res_0.bn1.running_var\", \"module.res_0.conv2.weight\", \"module.res_0.bn2.weight\", \"module.res_0.bn2.bias\", \"module.res_0.bn2.running_mean\", \"module.res_0.bn2.running_var\", \"module.res_1.conv1.weight\", \"module.res_1.bn1.weight\", \"module.res_1.bn1.bias\", \"module.res_1.bn1.running_mean\", \"module.res_1.bn1.running_var\", \"module.res_1.conv2.weight\", \"module.res_1.bn2.weight\", \"module.res_1.bn2.bias\", \"module.res_1.bn2.running_mean\", \"module.res_1.bn2.running_var\", \"module.res_2.conv1.weight\", \"module.res_2.bn1.weight\", \"module.res_2.bn1.bias\", \"module.res_2.bn1.running_mean\", \"module.res_2.bn1.running_var\", \"module.res_2.conv2.weight\", \"module.res_2.bn2.weight\", \"module.res_2.bn2.bias\", \"module.res_2.bn2.running_mean\", \"module.res_2.bn2.running_var\", \"module.res_3.conv1.weight\", \"module.res_3.bn1.weight\", \"module.res_3.bn1.bias\", \"module.res_3.bn1.running_mean\", \"module.res_3.bn1.running_var\", \"module.res_3.conv2.weight\", \"module.res_3.bn2.weight\", \"module.res_3.bn2.bias\", \"module.res_3.bn2.running_mean\", \"module.res_3.bn2.running_var\", \"module.res_4.conv1.weight\", \"module.res_4.bn1.weight\", \"module.res_4.bn1.bias\", \"module.res_4.bn1.running_mean\", \"module.res_4.bn1.running_var\", \"module.res_4.conv2.weight\", \"module.res_4.bn2.weight\", \"module.res_4.bn2.bias\", \"module.res_4.bn2.running_mean\", \"module.res_4.bn2.running_var\", \"module.res_5.conv1.weight\", \"module.res_5.bn1.weight\", \"module.res_5.bn1.bias\", \"module.res_5.bn1.running_mean\", \"module.res_5.bn1.running_var\", \"module.res_5.conv2.weight\", \"module.res_5.bn2.weight\", \"module.res_5.bn2.bias\", \"module.res_5.bn2.running_mean\", \"module.res_5.bn2.running_var\", \"module.res_6.conv1.weight\", \"module.res_6.bn1.weight\", \"module.res_6.bn1.bias\", \"module.res_6.bn1.running_mean\", \"module.res_6.bn1.running_var\", \"module.res_6.conv2.weight\", \"module.res_6.bn2.weight\", \"module.res_6.bn2.bias\", \"module.res_6.bn2.running_mean\", \"module.res_6.bn2.running_var\", \"module.res_7.conv1.weight\", \"module.res_7.bn1.weight\", \"module.res_7.bn1.bias\", \"module.res_7.bn1.running_mean\", \"module.res_7.bn1.running_var\", \"module.res_7.conv2.weight\", \"module.res_7.bn2.weight\", \"module.res_7.bn2.bias\", \"module.res_7.bn2.running_mean\", \"module.res_7.bn2.running_var\", \"module.res_8.conv1.weight\", \"module.res_8.bn1.weight\", \"module.res_8.bn1.bias\", \"module.res_8.bn1.running_mean\", \"module.res_8.bn1.running_var\", \"module.res_8.conv2.weight\", \"module.res_8.bn2.weight\", \"module.res_8.bn2.bias\", \"module.res_8.bn2.running_mean\", \"module.res_8.bn2.running_var\", \"module.res_9.conv1.weight\", \"module.res_9.bn1.weight\", \"module.res_9.bn1.bias\", \"module.res_9.bn1.running_mean\", \"module.res_9.bn1.running_var\", \"module.res_9.conv2.weight\", \"module.res_9.bn2.weight\", \"module.res_9.bn2.bias\", \"module.res_9.bn2.running_mean\", \"module.res_9.bn2.running_var\", \"module.res_10.conv1.weight\", \"module.res_10.bn1.weight\", \"module.res_10.bn1.bias\", \"module.res_10.bn1.running_mean\", \"module.res_10.bn1.running_var\", \"module.res_10.conv2.weight\", \"module.res_10.bn2.weight\", \"module.res_10.bn2.bias\", \"module.res_10.bn2.running_mean\", \"module.res_10.bn2.running_var\", \"module.res_11.conv1.weight\", \"module.res_11.bn1.weight\", \"module.res_11.bn1.bias\", \"module.res_11.bn1.running_mean\", \"module.res_11.bn1.running_var\", \"module.res_11.conv2.weight\", \"module.res_11.bn2.weight\", \"module.res_11.bn2.bias\", \"module.res_11.bn2.running_mean\", \"module.res_11.bn2.running_var\", \"module.res_12.conv1.weight\", \"module.res_12.bn1.weight\", \"module.res_12.bn1.bias\", \"module.res_12.bn1.running_mean\", \"module.res_12.bn1.running_var\", \"module.res_12.conv2.weight\", \"module.res_12.bn2.weight\", \"module.res_12.bn2.bias\", \"module.res_12.bn2.running_mean\", \"module.res_12.bn2.running_var\", \"module.res_13.conv1.weight\", \"module.res_13.bn1.weight\", \"module.res_13.bn1.bias\", \"module.res_13.bn1.running_mean\", \"module.res_13.bn1.running_var\", \"module.res_13.conv2.weight\", \"module.res_13.bn2.weight\", \"module.res_13.bn2.bias\", \"module.res_13.bn2.running_mean\", \"module.res_13.bn2.running_var\", \"module.res_14.conv1.weight\", \"module.res_14.bn1.weight\", \"module.res_14.bn1.bias\", \"module.res_14.bn1.running_mean\", \"module.res_14.bn1.running_var\", \"module.res_14.conv2.weight\", \"module.res_14.bn2.weight\", \"module.res_14.bn2.bias\", \"module.res_14.bn2.running_mean\", \"module.res_14.bn2.running_var\", \"module.res_15.conv1.weight\", \"module.res_15.bn1.weight\", \"module.res_15.bn1.bias\", \"module.res_15.bn1.running_mean\", \"module.res_15.bn1.running_var\", \"module.res_15.conv2.weight\", \"module.res_15.bn2.weight\", \"module.res_15.bn2.bias\", \"module.res_15.bn2.running_mean\", \"module.res_15.bn2.running_var\", \"module.res_16.conv1.weight\", \"module.res_16.bn1.weight\", \"module.res_16.bn1.bias\", \"module.res_16.bn1.running_mean\", \"module.res_16.bn1.running_var\", \"module.res_16.conv2.weight\", \"module.res_16.bn2.weight\", \"module.res_16.bn2.bias\", \"module.res_16.bn2.running_mean\", \"module.res_16.bn2.running_var\", \"module.res_17.conv1.weight\", \"module.res_17.bn1.weight\", \"module.res_17.bn1.bias\", \"module.res_17.bn1.running_mean\", \"module.res_17.bn1.running_var\", \"module.res_17.conv2.weight\", \"module.res_17.bn2.weight\", \"module.res_17.bn2.bias\", \"module.res_17.bn2.running_mean\", \"module.res_17.bn2.running_var\", \"module.res_18.conv1.weight\", \"module.res_18.bn1.weight\", \"module.res_18.bn1.bias\", \"module.res_18.bn1.running_mean\", \"module.res_18.bn1.running_var\", \"module.res_18.conv2.weight\", \"module.res_18.bn2.weight\", \"module.res_18.bn2.bias\", \"module.res_18.bn2.running_mean\", \"module.res_18.bn2.running_var\", \"module.res_19.conv1.weight\", \"module.res_19.bn1.weight\", \"module.res_19.bn1.bias\", \"module.res_19.bn1.running_mean\", \"module.res_19.bn1.running_var\", \"module.res_19.conv2.weight\", \"module.res_19.bn2.weight\", \"module.res_19.bn2.bias\", \"module.res_19.bn2.running_mean\", \"module.res_19.bn2.running_var\", \"module.res_20.conv1.weight\", \"module.res_20.bn1.weight\", \"module.res_20.bn1.bias\", \"module.res_20.bn1.running_mean\", \"module.res_20.bn1.running_var\", \"module.res_20.conv2.weight\", \"module.res_20.bn2.weight\", \"module.res_20.bn2.bias\", \"module.res_20.bn2.running_mean\", \"module.res_20.bn2.running_var\", \"module.res_21.conv1.weight\", \"module.res_21.bn1.weight\", \"module.res_21.bn1.bias\", \"module.res_21.bn1.running_mean\", \"module.res_21.bn1.running_var\", \"module.res_21.conv2.weight\", \"module.res_21.bn2.weight\", \"module.res_21.bn2.bias\", \"module.res_21.bn2.running_mean\", \"module.res_21.bn2.running_var\", \"module.res_22.conv1.weight\", \"module.res_22.bn1.weight\", \"module.res_22.bn1.bias\", \"module.res_22.bn1.running_mean\", \"module.res_22.bn1.running_var\", \"module.res_22.conv2.weight\", \"module.res_22.bn2.weight\", \"module.res_22.bn2.bias\", \"module.res_22.bn2.running_mean\", \"module.res_22.bn2.running_var\", \"module.res_23.conv1.weight\", \"module.res_23.bn1.weight\", \"module.res_23.bn1.bias\", \"module.res_23.bn1.running_mean\", \"module.res_23.bn1.running_var\", \"module.res_23.conv2.weight\", \"module.res_23.bn2.weight\", \"module.res_23.bn2.bias\", \"module.res_23.bn2.running_mean\", \"module.res_23.bn2.running_var\", \"module.res_24.conv1.weight\", \"module.res_24.bn1.weight\", \"module.res_24.bn1.bias\", \"module.res_24.bn1.running_mean\", \"module.res_24.bn1.running_var\", \"module.res_24.conv2.weight\", \"module.res_24.bn2.weight\", \"module.res_24.bn2.bias\", \"module.res_24.bn2.running_mean\", \"module.res_24.bn2.running_var\", \"module.res_25.conv1.weight\", \"module.res_25.bn1.weight\", \"module.res_25.bn1.bias\", \"module.res_25.bn1.running_mean\", \"module.res_25.bn1.running_var\", \"module.res_25.conv2.weight\", \"module.res_25.bn2.weight\", \"module.res_25.bn2.bias\", \"module.res_25.bn2.running_mean\", \"module.res_25.bn2.running_var\", \"module.res_26.conv1.weight\", \"module.res_26.bn1.weight\", \"module.res_26.bn1.bias\", \"module.res_26.bn1.running_mean\", \"module.res_26.bn1.running_var\", \"module.res_26.conv2.weight\", \"module.res_26.bn2.weight\", \"module.res_26.bn2.bias\", \"module.res_26.bn2.running_mean\", \"module.res_26.bn2.running_var\", \"module.res_27.conv1.weight\", \"module.res_27.bn1.weight\", \"module.res_27.bn1.bias\", \"module.res_27.bn1.running_mean\", \"module.res_27.bn1.running_var\", \"module.res_27.conv2.weight\", \"module.res_27.bn2.weight\", \"module.res_27.bn2.bias\", \"module.res_27.bn2.running_mean\", \"module.res_27.bn2.running_var\", \"module.res_28.conv1.weight\", \"module.res_28.bn1.weight\", \"module.res_28.bn1.bias\", \"module.res_28.bn1.running_mean\", \"module.res_28.bn1.running_var\", \"module.res_28.conv2.weight\", \"module.res_28.bn2.weight\", \"module.res_28.bn2.bias\", \"module.res_28.bn2.running_mean\", \"module.res_28.bn2.running_var\", \"module.res_29.conv1.weight\", \"module.res_29.bn1.weight\", \"module.res_29.bn1.bias\", \"module.res_29.bn1.running_mean\", \"module.res_29.bn1.running_var\", \"module.res_29.conv2.weight\", \"module.res_29.bn2.weight\", \"module.res_29.bn2.bias\", \"module.res_29.bn2.running_mean\", \"module.res_29.bn2.running_var\", \"module.outblock.fc1.weight\", \"module.outblock.fc1.bias\", \"module.outblock.fc2.weight\", \"module.outblock.fc2.bias\", \"module.outblock.fc.weight\", \"module.outblock.fc.bias\". \n\tUnexpected key(s) in state_dict: \"conv.conv1.weight\", \"conv.conv1.bias\", \"conv.bn1.weight\", \"conv.bn1.bias\", \"conv.bn1.running_mean\", \"conv.bn1.running_var\", \"conv.bn1.num_batches_tracked\", \"conv.conv2.weight\", \"conv.conv2.bias\", \"conv.bn2.weight\", \"conv.bn2.bias\", \"conv.bn2.running_mean\", \"conv.bn2.running_var\", \"conv.bn2.num_batches_tracked\", \"conv.conv3.weight\", \"conv.conv3.bias\", \"conv.bn3.weight\", \"conv.bn3.bias\", \"conv.bn3.running_mean\", \"conv.bn3.running_var\", \"conv.bn3.num_batches_tracked\", \"conv.conv4.weight\", \"conv.conv4.bias\", \"conv.bn4.weight\", \"conv.bn4.bias\", \"conv.bn4.running_mean\", \"conv.bn4.running_var\", \"conv.bn4.num_batches_tracked\", \"conv.conv5.weight\", \"conv.conv5.bias\", \"conv.bn5.weight\", \"conv.bn5.bias\", \"conv.bn5.running_mean\", \"conv.bn5.running_var\", \"conv.bn5.num_batches_tracked\", \"res_0.conv1.weight\", \"res_0.bn1.weight\", \"res_0.bn1.bias\", \"res_0.bn1.running_mean\", \"res_0.bn1.running_var\", \"res_0.bn1.num_batches_tracked\", \"res_0.conv2.weight\", \"res_0.bn2.weight\", \"res_0.bn2.bias\", \"res_0.bn2.running_mean\", \"res_0.bn2.running_var\", \"res_0.bn2.num_batches_tracked\", \"res_1.conv1.weight\", \"res_1.bn1.weight\", \"res_1.bn1.bias\", \"res_1.bn1.running_mean\", \"res_1.bn1.running_var\", \"res_1.bn1.num_batches_tracked\", \"res_1.conv2.weight\", \"res_1.bn2.weight\", \"res_1.bn2.bias\", \"res_1.bn2.running_mean\", \"res_1.bn2.running_var\", \"res_1.bn2.num_batches_tracked\", \"res_2.conv1.weight\", \"res_2.bn1.weight\", \"res_2.bn1.bias\", \"res_2.bn1.running_mean\", \"res_2.bn1.running_var\", \"res_2.bn1.num_batches_tracked\", \"res_2.conv2.weight\", \"res_2.bn2.weight\", \"res_2.bn2.bias\", \"res_2.bn2.running_mean\", \"res_2.bn2.running_var\", \"res_2.bn2.num_batches_tracked\", \"res_3.conv1.weight\", \"res_3.bn1.weight\", \"res_3.bn1.bias\", \"res_3.bn1.running_mean\", \"res_3.bn1.running_var\", \"res_3.bn1.num_batches_tracked\", \"res_3.conv2.weight\", \"res_3.bn2.weight\", \"res_3.bn2.bias\", \"res_3.bn2.running_mean\", \"res_3.bn2.running_var\", \"res_3.bn2.num_batches_tracked\", \"res_4.conv1.weight\", \"res_4.bn1.weight\", \"res_4.bn1.bias\", \"res_4.bn1.running_mean\", \"res_4.bn1.running_var\", \"res_4.bn1.num_batches_tracked\", \"res_4.conv2.weight\", \"res_4.bn2.weight\", \"res_4.bn2.bias\", \"res_4.bn2.running_mean\", \"res_4.bn2.running_var\", \"res_4.bn2.num_batches_tracked\", \"res_5.conv1.weight\", \"res_5.bn1.weight\", \"res_5.bn1.bias\", \"res_5.bn1.running_mean\", \"res_5.bn1.running_var\", \"res_5.bn1.num_batches_tracked\", \"res_5.conv2.weight\", \"res_5.bn2.weight\", \"res_5.bn2.bias\", \"res_5.bn2.running_mean\", \"res_5.bn2.running_var\", \"res_5.bn2.num_batches_tracked\", \"res_6.conv1.weight\", \"res_6.bn1.weight\", \"res_6.bn1.bias\", \"res_6.bn1.running_mean\", \"res_6.bn1.running_var\", \"res_6.bn1.num_batches_tracked\", \"res_6.conv2.weight\", \"res_6.bn2.weight\", \"res_6.bn2.bias\", \"res_6.bn2.running_mean\", \"res_6.bn2.running_var\", \"res_6.bn2.num_batches_tracked\", \"res_7.conv1.weight\", \"res_7.bn1.weight\", \"res_7.bn1.bias\", \"res_7.bn1.running_mean\", \"res_7.bn1.running_var\", \"res_7.bn1.num_batches_tracked\", \"res_7.conv2.weight\", \"res_7.bn2.weight\", \"res_7.bn2.bias\", \"res_7.bn2.running_mean\", \"res_7.bn2.running_var\", \"res_7.bn2.num_batches_tracked\", \"res_8.conv1.weight\", \"res_8.bn1.weight\", \"res_8.bn1.bias\", \"res_8.bn1.running_mean\", \"res_8.bn1.running_var\", \"res_8.bn1.num_batches_tracked\", \"res_8.conv2.weight\", \"res_8.bn2.weight\", \"res_8.bn2.bias\", \"res_8.bn2.running_mean\", \"res_8.bn2.running_var\", \"res_8.bn2.num_batches_tracked\", \"res_9.conv1.weight\", \"res_9.bn1.weight\", \"res_9.bn1.bias\", \"res_9.bn1.running_mean\", \"res_9.bn1.running_var\", \"res_9.bn1.num_batches_tracked\", \"res_9.conv2.weight\", \"res_9.bn2.weight\", \"res_9.bn2.bias\", \"res_9.bn2.running_mean\", \"res_9.bn2.running_var\", \"res_9.bn2.num_batches_tracked\", \"res_10.conv1.weight\", \"res_10.bn1.weight\", \"res_10.bn1.bias\", \"res_10.bn1.running_mean\", \"res_10.bn1.running_var\", \"res_10.bn1.num_batches_tracked\", \"res_10.conv2.weight\", \"res_10.bn2.weight\", \"res_10.bn2.bias\", \"res_10.bn2.running_mean\", \"res_10.bn2.running_var\", \"res_10.bn2.num_batches_tracked\", \"res_11.conv1.weight\", \"res_11.bn1.weight\", \"res_11.bn1.bias\", \"res_11.bn1.running_mean\", \"res_11.bn1.running_var\", \"res_11.bn1.num_batches_tracked\", \"res_11.conv2.weight\", \"res_11.bn2.weight\", \"res_11.bn2.bias\", \"res_11.bn2.running_mean\", \"res_11.bn2.running_var\", \"res_11.bn2.num_batches_tracked\", \"res_12.conv1.weight\", \"res_12.bn1.weight\", \"res_12.bn1.bias\", \"res_12.bn1.running_mean\", \"res_12.bn1.running_var\", \"res_12.bn1.num_batches_tracked\", \"res_12.conv2.weight\", \"res_12.bn2.weight\", \"res_12.bn2.bias\", \"res_12.bn2.running_mean\", \"res_12.bn2.running_var\", \"res_12.bn2.num_batches_tracked\", \"res_13.conv1.weight\", \"res_13.bn1.weight\", \"res_13.bn1.bias\", \"res_13.bn1.running_mean\", \"res_13.bn1.running_var\", \"res_13.bn1.num_batches_tracked\", \"res_13.conv2.weight\", \"res_13.bn2.weight\", \"res_13.bn2.bias\", \"res_13.bn2.running_mean\", \"res_13.bn2.running_var\", \"res_13.bn2.num_batches_tracked\", \"res_14.conv1.weight\", \"res_14.bn1.weight\", \"res_14.bn1.bias\", \"res_14.bn1.running_mean\", \"res_14.bn1.running_var\", \"res_14.bn1.num_batches_tracked\", \"res_14.conv2.weight\", \"res_14.bn2.weight\", \"res_14.bn2.bias\", \"res_14.bn2.running_mean\", \"res_14.bn2.running_var\", \"res_14.bn2.num_batches_tracked\", \"res_15.conv1.weight\", \"res_15.bn1.weight\", \"res_15.bn1.bias\", \"res_15.bn1.running_mean\", \"res_15.bn1.running_var\", \"res_15.bn1.num_batches_tracked\", \"res_15.conv2.weight\", \"res_15.bn2.weight\", \"res_15.bn2.bias\", \"res_15.bn2.running_mean\", \"res_15.bn2.running_var\", \"res_15.bn2.num_batches_tracked\", \"res_16.conv1.weight\", \"res_16.bn1.weight\", \"res_16.bn1.bias\", \"res_16.bn1.running_mean\", \"res_16.bn1.running_var\", \"res_16.bn1.num_batches_tracked\", \"res_16.conv2.weight\", \"res_16.bn2.weight\", \"res_16.bn2.bias\", \"res_16.bn2.running_mean\", \"res_16.bn2.running_var\", \"res_16.bn2.num_batches_tracked\", \"res_17.conv1.weight\", \"res_17.bn1.weight\", \"res_17.bn1.bias\", \"res_17.bn1.running_mean\", \"res_17.bn1.running_var\", \"res_17.bn1.num_batches_tracked\", \"res_17.conv2.weight\", \"res_17.bn2.weight\", \"res_17.bn2.bias\", \"res_17.bn2.running_mean\", \"res_17.bn2.running_var\", \"res_17.bn2.num_batches_tracked\", \"res_18.conv1.weight\", \"res_18.bn1.weight\", \"res_18.bn1.bias\", \"res_18.bn1.running_mean\", \"res_18.bn1.running_var\", \"res_18.bn1.num_batches_tracked\", \"res_18.conv2.weight\", \"res_18.bn2.weight\", \"res_18.bn2.bias\", \"res_18.bn2.running_mean\", \"res_18.bn2.running_var\", \"res_18.bn2.num_batches_tracked\", \"res_19.conv1.weight\", \"res_19.bn1.weight\", \"res_19.bn1.bias\", \"res_19.bn1.running_mean\", \"res_19.bn1.running_var\", \"res_19.bn1.num_batches_tracked\", \"res_19.conv2.weight\", \"res_19.bn2.weight\", \"res_19.bn2.bias\", \"res_19.bn2.running_mean\", \"res_19.bn2.running_var\", \"res_19.bn2.num_batches_tracked\", \"res_20.conv1.weight\", \"res_20.bn1.weight\", \"res_20.bn1.bias\", \"res_20.bn1.running_mean\", \"res_20.bn1.running_var\", \"res_20.bn1.num_batches_tracked\", \"res_20.conv2.weight\", \"res_20.bn2.weight\", \"res_20.bn2.bias\", \"res_20.bn2.running_mean\", \"res_20.bn2.running_var\", \"res_20.bn2.num_batches_tracked\", \"res_21.conv1.weight\", \"res_21.bn1.weight\", \"res_21.bn1.bias\", \"res_21.bn1.running_mean\", \"res_21.bn1.running_var\", \"res_21.bn1.num_batches_tracked\", \"res_21.conv2.weight\", \"res_21.bn2.weight\", \"res_21.bn2.bias\", \"res_21.bn2.running_mean\", \"res_21.bn2.running_var\", \"res_21.bn2.num_batches_tracked\", \"res_22.conv1.weight\", \"res_22.bn1.weight\", \"res_22.bn1.bias\", \"res_22.bn1.running_mean\", \"res_22.bn1.running_var\", \"res_22.bn1.num_batches_tracked\", \"res_22.conv2.weight\", \"res_22.bn2.weight\", \"res_22.bn2.bias\", \"res_22.bn2.running_mean\", \"res_22.bn2.running_var\", \"res_22.bn2.num_batches_tracked\", \"res_23.conv1.weight\", \"res_23.bn1.weight\", \"res_23.bn1.bias\", \"res_23.bn1.running_mean\", \"res_23.bn1.running_var\", \"res_23.bn1.num_batches_tracked\", \"res_23.conv2.weight\", \"res_23.bn2.weight\", \"res_23.bn2.bias\", \"res_23.bn2.running_mean\", \"res_23.bn2.running_var\", \"res_23.bn2.num_batches_tracked\", \"res_24.conv1.weight\", \"res_24.bn1.weight\", \"res_24.bn1.bias\", \"res_24.bn1.running_mean\", \"res_24.bn1.running_var\", \"res_24.bn1.num_batches_tracked\", \"res_24.conv2.weight\", \"res_24.bn2.weight\", \"res_24.bn2.bias\", \"res_24.bn2.running_mean\", \"res_24.bn2.running_var\", \"res_24.bn2.num_batches_tracked\", \"res_25.conv1.weight\", \"res_25.bn1.weight\", \"res_25.bn1.bias\", \"res_25.bn1.running_mean\", \"res_25.bn1.running_var\", \"res_25.bn1.num_batches_tracked\", \"res_25.conv2.weight\", \"res_25.bn2.weight\", \"res_25.bn2.bias\", \"res_25.bn2.running_mean\", \"res_25.bn2.running_var\", \"res_25.bn2.num_batches_tracked\", \"res_26.conv1.weight\", \"res_26.bn1.weight\", \"res_26.bn1.bias\", \"res_26.bn1.running_mean\", \"res_26.bn1.running_var\", \"res_26.bn1.num_batches_tracked\", \"res_26.conv2.weight\", \"res_26.bn2.weight\", \"res_26.bn2.bias\", \"res_26.bn2.running_mean\", \"res_26.bn2.running_var\", \"res_26.bn2.num_batches_tracked\", \"res_27.conv1.weight\", \"res_27.bn1.weight\", \"res_27.bn1.bias\", \"res_27.bn1.running_mean\", \"res_27.bn1.running_var\", \"res_27.bn1.num_batches_tracked\", \"res_27.conv2.weight\", \"res_27.bn2.weight\", \"res_27.bn2.bias\", \"res_27.bn2.running_mean\", \"res_27.bn2.running_var\", \"res_27.bn2.num_batches_tracked\", \"res_28.conv1.weight\", \"res_28.bn1.weight\", \"res_28.bn1.bias\", \"res_28.bn1.running_mean\", \"res_28.bn1.running_var\", \"res_28.bn1.num_batches_tracked\", \"res_28.conv2.weight\", \"res_28.bn2.weight\", \"res_28.bn2.bias\", \"res_28.bn2.running_mean\", \"res_28.bn2.running_var\", \"res_28.bn2.num_batches_tracked\", \"res_29.conv1.weight\", \"res_29.bn1.weight\", \"res_29.bn1.bias\", \"res_29.bn1.running_mean\", \"res_29.bn1.running_var\", \"res_29.bn1.num_batches_tracked\", \"res_29.conv2.weight\", \"res_29.bn2.weight\", \"res_29.bn2.bias\", \"res_29.bn2.running_mean\", \"res_29.bn2.running_var\", \"res_29.bn2.num_batches_tracked\", \"outblock.fc1.weight\", \"outblock.fc1.bias\", \"outblock.fc2.weight\", \"outblock.fc2.bias\", \"outblock.fc.weight\", \"outblock.fc.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-bbfff559baac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloadbrain1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# loadbrain2()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-3e1a93d56279>\u001b[0m in \u001b[0;36mloadbrain1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=> loading checkpoint... \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bestrandom.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcuda0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1224\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DataParallel:\n\tMissing key(s) in state_dict: \"module.conv.conv1.weight\", \"module.conv.conv1.bias\", \"module.conv.bn1.weight\", \"module.conv.bn1.bias\", \"module.conv.bn1.running_mean\", \"module.conv.bn1.running_var\", \"module.conv.conv2.weight\", \"module.conv.conv2.bias\", \"module.conv.bn2.weight\", \"module.conv.bn2.bias\", \"module.conv.bn2.running_mean\", \"module.conv.bn2.running_var\", \"module.conv.conv3.weight\", \"module.conv.conv3.bias\", \"module.conv.bn3.weight\", \"module.conv.bn3.bias\", \"module.conv.bn3.running_mean\", \"module.conv.bn3.running_var\", \"module.conv.conv4.weight\", \"module.conv.conv4.bias\", \"module.conv.bn4.weight\", \"module.conv.bn4.bias\", \"module.conv.bn4.running_mean\", \"module.conv.bn4.running_var\", \"module.conv.conv5.weight\", \"module.conv.conv5.bias\", \"module.conv.bn5.weight\", \"module.conv.bn5.bias\", \"module.conv.bn5.running_mean\", \"module.conv.bn5.running_var\", \"module.res_0.conv1.weight\", \"module.res_0.bn1.weight\", \"module.res_0.bn1.bias\", \"module.res_0.bn1.running_mean\", \"module.res_0.bn1.running_var\", \"module.res_0.conv2.weight\", \"module.res_0.bn2.weight\", \"module.res_0.bn2.bias\", \"module.res_0.bn2.running_mean\", \"module.res_0.bn2.running_var\", \"module.res_1.conv1.weight\", \"module.res_1.bn1.weight\", \"module.res_1.bn1.bias\", \"module.res_1.bn1.running_mean\", \"module.res_1.bn1.running_var\", \"module.res_1.conv2.weight\", \"module.res_1.bn2.weight\", \"module.res_1.bn2.bias\", \"module.res_1.bn2.running_mean\", \"module.res_1.bn2.running_var\", \"module.res_2.conv1.weight\", \"module.res_2.bn1.weight\", \"module.res_2.bn1.bias\", \"module.res_2.bn1.running_mean\", \"module.res_2.bn1.running_var\", \"module.res_2.conv2.weight\", \"module.res_2.bn2.weight\", \"module.res_2.bn2.bias\", \"module.res_2.bn2.running_mean\", \"module.res_2.bn2.running_var\", \"module.res_3.conv1.weight\", \"module.res_3.bn1.weight\", \"module.res_3.bn1.bias\", \"module.res_3.bn1.running_mean\", \"module.res_3.bn1.running_var\", \"module.res_3.conv2.weight\", \"module.res_3.bn2.weight\", \"module.res_3.bn2.bias\", \"module.res_3.bn2.running_mean\", \"module.res_3.bn2.running_var\", \"module.res_4.conv1.weight\", \"module.res_4.bn1.weight\", \"module.res_4.bn1.bias\", \"module.res_4.bn1.running_mean\", \"module.res_4.bn1.running_var\", \"module.res_4.conv2.weight\", \"module.res_4.bn2.weight\", \"module.res_4.bn2.bias\", \"module.res_4.bn2.running_mean\", \"module.res_4.bn2.running_var\", \"module.res_5.conv1.weight\", \"module.res_5.bn1.weight\", \"module.res_5.bn1.bias\", \"module.res_5.bn1.running_mean\", \"module.res_5.bn1.running_var\", \"module.res_5.conv2.weight\", \"module.res_5.bn2.weight\", \"module.res_5.bn2.bias\", \"module.res_5.bn2.running_mean\", \"module.res_5.bn2.running_var\", \"module.res_6.conv1.weight\", \"module.res_6.bn1.weight\", \"module.res_6.bn1.bias\", \"module.res_6.bn1.running_mean\", \"module.res_6.bn1.running_var\", \"module.res_6.conv2.weight\", \"module.res_6.bn2.weight\", \"module.res_6.bn2.bias\", \"module.res_6.bn2.running_mean\", \"module.res_6.bn2.running_var\", \"module.res_7.conv1.weight\", \"module.res_7.bn1.weight\", \"module.res_7.bn1.bias\", \"module.res_7.bn1.running_mean\", \"module.res_7.bn1.running_var\", \"module.res_7.conv2.weight\", \"module.res_7.bn2.weight\", \"module.res_7.bn2.bias\", \"module.res_7.bn2.running_mean\", \"module.res_7.bn2.running_var\", \"module.res_8.conv1.weight\", \"module.res_8.bn1.weight\", \"module.res_8.bn1.bias\", \"module.res_8.bn1.running_mean\", \"module.res_8.bn1.running_var\", \"module.res_8.conv2.weight\", \"module.res_8.bn2.weight\", \"module.res_8.bn2.bias\", \"module.res_8.bn2.running_mean\", \"module.res_8.bn2.running_var\", \"module.res_9.conv1.weight\", \"module.res_9.bn1.weight\", \"module.res_9.bn1.bias\", \"module.res_9.bn1.running_mean\", \"module.res_9.bn1.running_var\", \"module.res_9.conv2.weight\", \"module.res_9.bn2.weight\", \"module.res_9.bn2.bias\", \"module.res_9.bn2.running_mean\", \"module.res_9.bn2.running_var\", \"module.res_10.conv1.weight\", \"module.res_10.bn1.weight\", \"module.res_10.bn1.bias\", \"module.res_10.bn1.running_mean\", \"module.res_10.bn1.running_var\", \"module.res_10.conv2.weight\", \"module.res_10.bn2.weight\", \"module.res_10.bn2.bias\", \"module.res_10.bn2.running_mean\", \"module.res_10.bn2.running_var\", \"module.res_11.conv1.weight\", \"module.res_11.bn1.weight\", \"module.res_11.bn1.bias\", \"module.res_11.bn1.running_mean\", \"module.res_11.bn1.running_var\", \"module.res_11.conv2.weight\", \"module.res_11.bn2.weight\", \"module.res_11.bn2.bias\", \"module.res_11.bn2.running_mean\", \"module.res_11.bn2.running_var\", \"module.res_12.conv1.weight\", \"module.res_12.bn1.weight\", \"module.res_12.bn1.bias\", \"module.res_12.bn1.running_mean\", \"module.res_12.bn1.running_var\", \"module.res_12.conv2.weight\", \"module.res_12.bn2.weight\", \"module.res_12.bn2.bias\", \"module.res_12.bn2.running_mean\", \"module.res_12.bn2.running_var\", \"module.res_13.conv1.weight\", \"module.res_13.bn1.weight\", \"module.res_13.bn1.bias\", \"module.res_13.bn1.running_mean\", \"module.res_13.bn1.running_var\", \"module.res_13.conv2.weight\", \"module.res_13.bn2.weight\", \"module.res_13.bn2.bias\", \"module.res_13.bn2.running_mean\", \"module.res_13.bn2.running_var\", \"module.res_14.conv1.weight\", \"module.res_14.bn1.weight\", \"module.res_14.bn1.bias\", \"module.res_14.bn1.running_mean\", \"module.res_14.bn1.running_var\", \"module.res_14.conv2.weight\", \"module.res_14.bn2.weight\", \"module.res_14.bn2.bias\", \"module.res_14.bn2.running_mean\", \"module.res_14.bn2.running_var\", \"module.res_15.conv1.weight\", \"module.res_15.bn1.weight\", \"module.res_15.bn1.bias\", \"module.res_15.bn1.running_mean\", \"module.res_15.bn1.running_var\", \"module.res_15.conv2.weight\", \"module.res_15.bn2.weight\", \"module.res_15.bn2.bias\", \"module.res_15.bn2.running_mean\", \"module.res_15.bn2.running_var\", \"module.res_16.conv1.weight\", \"module.res_16.bn1.weight\", \"module.res_16.bn1.bias\", \"module.res_16.bn1.running_mean\", \"module.res_16.bn1.running_var\", \"module.res_16.conv2.weight\", \"module.res_16.bn2.weight\", \"module.res_16.bn2.bias\", \"module.res_16.bn2.running_mean\", \"module.res_16.bn2.running_var\", \"module.res_17.conv1.weight\", \"module.res_17.bn1.weight\", \"module.res_17.bn1.bias\", \"module.res_17.bn1.running_mean\", \"module.res_17.bn1.running_var\", \"module.res_17.conv2.weight\", \"module.res_17.bn2.weight\", \"module.res_17.bn2.bias\", \"module.res_17.bn2.running_mean\", \"module.res_17.bn2.running_var\", \"module.res_18.conv1.weight\", \"module.res_18.bn1.weight\", \"module.res_18.bn1.bias\", \"module.res_18.bn1.running_mean\", \"module.res_18.bn1.running_var\", \"module.res_18.conv2.weight\", \"module.res_18.bn2.weight\", \"module.res_18.bn2.bias\", \"module.res_18.bn2.running_mean\", \"module.res_18.bn2.running_var\", \"module.res_19.conv1.weight\", \"module.res_19.bn1.weight\", \"module.res_19.bn1.bias\", \"module.res_19.bn1.running_mean\", \"module.res_19.bn1.running_var\", \"module.res_19.conv2.weight\", \"module.res_19.bn2.weight\", \"module.res_19.bn2.bias\", \"module.res_19.bn2.running_mean\", \"module.res_19.bn2.running_var\", \"module.res_20.conv1.weight\", \"module.res_20.bn1.weight\", \"module.res_20.bn1.bias\", \"module.res_20.bn1.running_mean\", \"module.res_20.bn1.running_var\", \"module.res_20.conv2.weight\", \"module.res_20.bn2.weight\", \"module.res_20.bn2.bias\", \"module.res_20.bn2.running_mean\", \"module.res_20.bn2.running_var\", \"module.res_21.conv1.weight\", \"module.res_21.bn1.weight\", \"module.res_21.bn1.bias\", \"module.res_21.bn1.running_mean\", \"module.res_21.bn1.running_var\", \"module.res_21.conv2.weight\", \"module.res_21.bn2.weight\", \"module.res_21.bn2.bias\", \"module.res_21.bn2.running_mean\", \"module.res_21.bn2.running_var\", \"module.res_22.conv1.weight\", \"module.res_22.bn1.weight\", \"module.res_22.bn1.bias\", \"module.res_22.bn1.running_mean\", \"module.res_22.bn1.running_var\", \"module.res_22.conv2.weight\", \"module.res_22.bn2.weight\", \"module.res_22.bn2.bias\", \"module.res_22.bn2.running_mean\", \"module.res_22.bn2.running_var\", \"module.res_23.conv1.weight\", \"module.res_23.bn1.weight\", \"module.res_23.bn1.bias\", \"module.res_23.bn1.running_mean\", \"module.res_23.bn1.running_var\", \"module.res_23.conv2.weight\", \"module.res_23.bn2.weight\", \"module.res_23.bn2.bias\", \"module.res_23.bn2.running_mean\", \"module.res_23.bn2.running_var\", \"module.res_24.conv1.weight\", \"module.res_24.bn1.weight\", \"module.res_24.bn1.bias\", \"module.res_24.bn1.running_mean\", \"module.res_24.bn1.running_var\", \"module.res_24.conv2.weight\", \"module.res_24.bn2.weight\", \"module.res_24.bn2.bias\", \"module.res_24.bn2.running_mean\", \"module.res_24.bn2.running_var\", \"module.res_25.conv1.weight\", \"module.res_25.bn1.weight\", \"module.res_25.bn1.bias\", \"module.res_25.bn1.running_mean\", \"module.res_25.bn1.running_var\", \"module.res_25.conv2.weight\", \"module.res_25.bn2.weight\", \"module.res_25.bn2.bias\", \"module.res_25.bn2.running_mean\", \"module.res_25.bn2.running_var\", \"module.res_26.conv1.weight\", \"module.res_26.bn1.weight\", \"module.res_26.bn1.bias\", \"module.res_26.bn1.running_mean\", \"module.res_26.bn1.running_var\", \"module.res_26.conv2.weight\", \"module.res_26.bn2.weight\", \"module.res_26.bn2.bias\", \"module.res_26.bn2.running_mean\", \"module.res_26.bn2.running_var\", \"module.res_27.conv1.weight\", \"module.res_27.bn1.weight\", \"module.res_27.bn1.bias\", \"module.res_27.bn1.running_mean\", \"module.res_27.bn1.running_var\", \"module.res_27.conv2.weight\", \"module.res_27.bn2.weight\", \"module.res_27.bn2.bias\", \"module.res_27.bn2.running_mean\", \"module.res_27.bn2.running_var\", \"module.res_28.conv1.weight\", \"module.res_28.bn1.weight\", \"module.res_28.bn1.bias\", \"module.res_28.bn1.running_mean\", \"module.res_28.bn1.running_var\", \"module.res_28.conv2.weight\", \"module.res_28.bn2.weight\", \"module.res_28.bn2.bias\", \"module.res_28.bn2.running_mean\", \"module.res_28.bn2.running_var\", \"module.res_29.conv1.weight\", \"module.res_29.bn1.weight\", \"module.res_29.bn1.bias\", \"module.res_29.bn1.running_mean\", \"module.res_29.bn1.running_var\", \"module.res_29.conv2.weight\", \"module.res_29.bn2.weight\", \"module.res_29.bn2.bias\", \"module.res_29.bn2.running_mean\", \"module.res_29.bn2.running_var\", \"module.outblock.fc1.weight\", \"module.outblock.fc1.bias\", \"module.outblock.fc2.weight\", \"module.outblock.fc2.bias\", \"module.outblock.fc.weight\", \"module.outblock.fc.bias\". \n\tUnexpected key(s) in state_dict: \"conv.conv1.weight\", \"conv.conv1.bias\", \"conv.bn1.weight\", \"conv.bn1.bias\", \"conv.bn1.running_mean\", \"conv.bn1.running_var\", \"conv.bn1.num_batches_tracked\", \"conv.conv2.weight\", \"conv.conv2.bias\", \"conv.bn2.weight\", \"conv.bn2.bias\", \"conv.bn2.running_mean\", \"conv.bn2.running_var\", \"conv.bn2.num_batches_tracked\", \"conv.conv3.weight\", \"conv.conv3.bias\", \"conv.bn3.weight\", \"conv.bn3.bias\", \"conv.bn3.running_mean\", \"conv.bn3.running_var\", \"conv.bn3.num_batches_tracked\", \"conv.conv4.weight\", \"conv.conv4.bias\", \"conv.bn4.weight\", \"conv.bn4.bias\", \"conv.bn4.running_mean\", \"conv.bn4.running_var\", \"conv.bn4.num_batches_tracked\", \"conv.conv5.weight\", \"conv.conv5.bias\", \"conv.bn5.weight\", \"conv.bn5.bias\", \"conv.bn5.running_mean\", \"conv.bn5.running_var\", \"conv.bn5.num_batches_tracked\", \"res_0.conv1.weight\", \"res_0.bn1.weight\", \"res_0.bn1.bias\", \"res_0.bn1.running_mean\", \"res_0.bn1.running_var\", \"res_0.bn1.num_batches_tracked\", \"res_0.conv2.weight\", \"res_0.bn2.weight\", \"res_0.bn2.bias\", \"res_0.bn2.running_mean\", \"res_0.bn2.running_var\", \"res_0.bn2.num_batches_tracked\", \"res_1.conv1.weight\", \"res_1.bn1.weight\", \"res_1.bn1.bias\", \"res_1.bn1.running_mean\", \"res_1.bn1.running_var\", \"res_1.bn1.num_batches_tracked\", \"res_1.conv2.weight\", \"res_1.bn2.weight\", \"res_1.bn2.bias\", \"res_1.bn2.running_mean\", \"res_1.bn2.running_var\", \"res_1.bn2.num_batches_tracked\", \"res_2.conv1.weight\", \"res_2.bn1.weight\", \"res_2.bn1.bias\", \"res_2.bn1.running_mean\", \"res_2.bn1.running_var\", \"res_2.bn1.num_batches_tracked\", \"res_2.conv2.weight\", \"res_2.bn2.weight\", \"res_2.bn2.bias\", \"res_2.bn2.running_mean\", \"res_2.bn2.running_var\", \"res_2.bn2.num_batches_tracked\", \"res_3.conv1.weight\", \"res_3.bn1.weight\", \"res_3.bn1.bias\", \"res_3.bn1.running_mean\", \"res_3.bn1.running_var\", \"res_3.bn1.num_batches_tracked\", \"res_3.conv2.weight\", \"res_3.bn2.weight\", \"res_3.bn2.bias\", \"res_3.bn2.running_mean\", \"res_3.bn2.running_var\", \"res_3.bn2.num_batches_tracked\", \"res_4.conv1.weight\", \"res_4.bn1.weight\", \"res_4.bn1.bias\", \"res_4.bn1.running_mean\", \"res_4.bn1.running_var\", \"res_4.bn1.num_batches_tracked\", \"res_4.conv2.weight\", \"res_4.bn2.weight\", \"res_4.bn2.bias\", \"res_4.bn2.running_mean\", \"res_4.bn2.running_var\", \"res_4.bn2.num_batches_tracked\", \"res_5.conv1.weight\", \"res_5.bn1.weight\", \"res_5.bn1.bias\", \"res_5.bn1.running_mean\", \"res_5.bn1.running_var\", \"res_5.bn1.num_batches_tracked\", \"res_5.conv2.weight\", \"res_5.bn2.weight\", \"res_5.bn2.bias\", \"res_5.bn2.running_mean\", \"res_5.bn2.running_var\", \"res_5.bn2.num_batches_tracked\", \"res_6.conv1.weight\", \"res_6.bn1.weight\", \"res_6.bn1.bias\", \"res_6.bn1.running_mean\", \"res_6.bn1.running_var\", \"res_6.bn1.num_batches_tracked\", \"res_6.conv2.weight\", \"res_6.bn2.weight\", \"res_6.bn2.bias\", \"res_6.bn2.running_mean\", \"res_6.bn2.running_var\", \"res_6.bn2.num_batches_tracked\", \"res_7.conv1.weight\", \"res_7.bn1.weight\", \"res_7.bn1.bias\", \"res_7.bn1.running_mean\", \"res_7.bn1.running_var\", \"res_7.bn1.num_batches_tracked\", \"res_7.conv2.weight\", \"res_7.bn2.weight\", \"res_7.bn2.bias\", \"res_7.bn2.running_mean\", \"res_7.bn2.running_var\", \"res_7.bn2.num_batches_tracked\", \"res_8.conv1.weight\", \"res_8.bn1.weight\", \"res_8.bn1.bias\", \"res_8.bn1.running_mean\", \"res_8.bn1.running_var\", \"res_8.bn1.num_batches_tracked\", \"res_8.conv2.weight\", \"res_8.bn2.weight\", \"res_8.bn2.bias\", \"res_8.bn2.running_mean\", \"res_8.bn2.running_var\", \"res_8.bn2.num_batches_tracked\", \"res_9.conv1.weight\", \"res_9.bn1.weight\", \"res_9.bn1.bias\", \"res_9.bn1.running_mean\", \"res_9.bn1.running_var\", \"res_9.bn1.num_batches_tracked\", \"res_9.conv2.weight\", \"res_9.bn2.weight\", \"res_9.bn2.bias\", \"res_9.bn2.running_mean\", \"res_9.bn2.running_var\", \"res_9.bn2.num_batches_tracked\", \"res_10.conv1.weight\", \"res_10.bn1.weight\", \"res_10.bn1.bias\", \"res_10.bn1.running_mean\", \"res_10.bn1.running_var\", \"res_10.bn1.num_batches_tracked\", \"res_10.conv2.weight\", \"res_10.bn2.weight\", \"res_10.bn2.bias\", \"res_10.bn2.running_mean\", \"res_10.bn2.running_var\", \"res_10.bn2.num_batches_tracked\", \"res_11.conv1.weight\", \"res_11.bn1.weight\", \"res_11.bn1.bias\", \"res_11.bn1.running_mean\", \"res_11.bn1.running_var\", \"res_11.bn1.num_batches_tracked\", \"res_11.conv2.weight\", \"res_11.bn2.weight\", \"res_11.bn2.bias\", \"res_11.bn2.running_mean\", \"res_11.bn2.running_var\", \"res_11.bn2.num_batches_tracked\", \"res_12.conv1.weight\", \"res_12.bn1.weight\", \"res_12.bn1.bias\", \"res_12.bn1.running_mean\", \"res_12.bn1.running_var\", \"res_12.bn1.num_batches_tracked\", \"res_12.conv2.weight\", \"res_12.bn2.weight\", \"res_12.bn2.bias\", \"res_12.bn2.running_mean\", \"res_12.bn2.running_var\", \"res_12.bn2.num_batches_tracked\", \"res_13.conv1.weight\", \"res_13.bn1.weight\", \"res_13.bn1.bias\", \"res_13.bn1.running_mean\", \"res_13.bn1.running_var\", \"res_13.bn1.num_batches_tracked\", \"res_13.conv2.weight\", \"res_13.bn2.weight\", \"res_13.bn2.bias\", \"res_13.bn2.running_mean\", \"res_13.bn2.running_var\", \"res_13.bn2.num_batches_tracked\", \"res_14.conv1.weight\", \"res_14.bn1.weight\", \"res_14.bn1.bias\", \"res_14.bn1.running_mean\", \"res_14.bn1.running_var\", \"res_14.bn1.num_batches_tracked\", \"res_14.conv2.weight\", \"res_14.bn2.weight\", \"res_14.bn2.bias\", \"res_14.bn2.running_mean\", \"res_14.bn2.running_var\", \"res_14.bn2.num_batches_tracked\", \"res_15.conv1.weight\", \"res_15.bn1.weight\", \"res_15.bn1.bias\", \"res_15.bn1.running_mean\", \"res_15.bn1.running_var\", \"res_15.bn1.num_batches_tracked\", \"res_15.conv2.weight\", \"res_15.bn2.weight\", \"res_15.bn2.bias\", \"res_15.bn2.running_mean\", \"res_15.bn2.running_var\", \"res_15.bn2.num_batches_tracked\", \"res_16.conv1.weight\", \"res_16.bn1.weight\", \"res_16.bn1.bias\", \"res_16.bn1.running_mean\", \"res_16.bn1.running_var\", \"res_16.bn1.num_batches_tracked\", \"res_16.conv2.weight\", \"res_16.bn2.weight\", \"res_16.bn2.bias\", \"res_16.bn2.running_mean\", \"res_16.bn2.running_var\", \"res_16.bn2.num_batches_tracked\", \"res_17.conv1.weight\", \"res_17.bn1.weight\", \"res_17.bn1.bias\", \"res_17.bn1.running_mean\", \"res_17.bn1.running_var\", \"res_17.bn1.num_batches_tracked\", \"res_17.conv2.weight\", \"res_17.bn2.weight\", \"res_17.bn2.bias\", \"res_17.bn2.running_mean\", \"res_17.bn2.running_var\", \"res_17.bn2.num_batches_tracked\", \"res_18.conv1.weight\", \"res_18.bn1.weight\", \"res_18.bn1.bias\", \"res_18.bn1.running_mean\", \"res_18.bn1.running_var\", \"res_18.bn1.num_batches_tracked\", \"res_18.conv2.weight\", \"res_18.bn2.weight\", \"res_18.bn2.bias\", \"res_18.bn2.running_mean\", \"res_18.bn2.running_var\", \"res_18.bn2.num_batches_tracked\", \"res_19.conv1.weight\", \"res_19.bn1.weight\", \"res_19.bn1.bias\", \"res_19.bn1.running_mean\", \"res_19.bn1.running_var\", \"res_19.bn1.num_batches_tracked\", \"res_19.conv2.weight\", \"res_19.bn2.weight\", \"res_19.bn2.bias\", \"res_19.bn2.running_mean\", \"res_19.bn2.running_var\", \"res_19.bn2.num_batches_tracked\", \"res_20.conv1.weight\", \"res_20.bn1.weight\", \"res_20.bn1.bias\", \"res_20.bn1.running_mean\", \"res_20.bn1.running_var\", \"res_20.bn1.num_batches_tracked\", \"res_20.conv2.weight\", \"res_20.bn2.weight\", \"res_20.bn2.bias\", \"res_20.bn2.running_mean\", \"res_20.bn2.running_var\", \"res_20.bn2.num_batches_tracked\", \"res_21.conv1.weight\", \"res_21.bn1.weight\", \"res_21.bn1.bias\", \"res_21.bn1.running_mean\", \"res_21.bn1.running_var\", \"res_21.bn1.num_batches_tracked\", \"res_21.conv2.weight\", \"res_21.bn2.weight\", \"res_21.bn2.bias\", \"res_21.bn2.running_mean\", \"res_21.bn2.running_var\", \"res_21.bn2.num_batches_tracked\", \"res_22.conv1.weight\", \"res_22.bn1.weight\", \"res_22.bn1.bias\", \"res_22.bn1.running_mean\", \"res_22.bn1.running_var\", \"res_22.bn1.num_batches_tracked\", \"res_22.conv2.weight\", \"res_22.bn2.weight\", \"res_22.bn2.bias\", \"res_22.bn2.running_mean\", \"res_22.bn2.running_var\", \"res_22.bn2.num_batches_tracked\", \"res_23.conv1.weight\", \"res_23.bn1.weight\", \"res_23.bn1.bias\", \"res_23.bn1.running_mean\", \"res_23.bn1.running_var\", \"res_23.bn1.num_batches_tracked\", \"res_23.conv2.weight\", \"res_23.bn2.weight\", \"res_23.bn2.bias\", \"res_23.bn2.running_mean\", \"res_23.bn2.running_var\", \"res_23.bn2.num_batches_tracked\", \"res_24.conv1.weight\", \"res_24.bn1.weight\", \"res_24.bn1.bias\", \"res_24.bn1.running_mean\", \"res_24.bn1.running_var\", \"res_24.bn1.num_batches_tracked\", \"res_24.conv2.weight\", \"res_24.bn2.weight\", \"res_24.bn2.bias\", \"res_24.bn2.running_mean\", \"res_24.bn2.running_var\", \"res_24.bn2.num_batches_tracked\", \"res_25.conv1.weight\", \"res_25.bn1.weight\", \"res_25.bn1.bias\", \"res_25.bn1.running_mean\", \"res_25.bn1.running_var\", \"res_25.bn1.num_batches_tracked\", \"res_25.conv2.weight\", \"res_25.bn2.weight\", \"res_25.bn2.bias\", \"res_25.bn2.running_mean\", \"res_25.bn2.running_var\", \"res_25.bn2.num_batches_tracked\", \"res_26.conv1.weight\", \"res_26.bn1.weight\", \"res_26.bn1.bias\", \"res_26.bn1.running_mean\", \"res_26.bn1.running_var\", \"res_26.bn1.num_batches_tracked\", \"res_26.conv2.weight\", \"res_26.bn2.weight\", \"res_26.bn2.bias\", \"res_26.bn2.running_mean\", \"res_26.bn2.running_var\", \"res_26.bn2.num_batches_tracked\", \"res_27.conv1.weight\", \"res_27.bn1.weight\", \"res_27.bn1.bias\", \"res_27.bn1.running_mean\", \"res_27.bn1.running_var\", \"res_27.bn1.num_batches_tracked\", \"res_27.conv2.weight\", \"res_27.bn2.weight\", \"res_27.bn2.bias\", \"res_27.bn2.running_mean\", \"res_27.bn2.running_var\", \"res_27.bn2.num_batches_tracked\", \"res_28.conv1.weight\", \"res_28.bn1.weight\", \"res_28.bn1.bias\", \"res_28.bn1.running_mean\", \"res_28.bn1.running_var\", \"res_28.bn1.num_batches_tracked\", \"res_28.conv2.weight\", \"res_28.bn2.weight\", \"res_28.bn2.bias\", \"res_28.bn2.running_mean\", \"res_28.bn2.running_var\", \"res_28.bn2.num_batches_tracked\", \"res_29.conv1.weight\", \"res_29.bn1.weight\", \"res_29.bn1.bias\", \"res_29.bn1.running_mean\", \"res_29.bn1.running_var\", \"res_29.bn1.num_batches_tracked\", \"res_29.conv2.weight\", \"res_29.bn2.weight\", \"res_29.bn2.bias\", \"res_29.bn2.running_mean\", \"res_29.bn2.running_var\", \"res_29.bn2.num_batches_tracked\", \"outblock.fc1.weight\", \"outblock.fc1.bias\", \"outblock.fc2.weight\", \"outblock.fc2.bias\", \"outblock.fc.weight\", \"outblock.fc.bias\". "
     ]
    }
   ],
   "source": [
    "\n",
    "loadbrain1()\n",
    "# loadbrain2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lr3T4CA7MuXj"
   },
   "outputs": [],
   "source": [
    "class MCTS_play:\n",
    "\n",
    "    def backpropagate(self, search_path, value, to_play):\n",
    "        \"\"\"\n",
    "        At the end of a simulation, we propagate the evaluation all the way up the tree\n",
    "        to the root.\n",
    "        \"\"\"\n",
    "        for node in reversed(search_path):\n",
    "            node.value_sum += value if node.to_play == to_play else -value\n",
    "            node.visit_count += 1\n",
    "\n",
    "    def run(self, state, to_play):\n",
    "      with torch.no_grad():      \n",
    "          gridnormeOne = np.zeros(shape=(6, 7))\n",
    "          gridnormenegOne = np.zeros(shape=(6, 7))\n",
    "          gridnormeZero = np.zeros(shape=(6, 7))\n",
    "          root = Node(0, to_play)\n",
    "          convert_zero(state, gridnormeZero)\n",
    "          convert_one(state, gridnormeOne)\n",
    "          convert_neg_one(state, gridnormenegOne)\n",
    "          gridAll = [gridnormeZero, gridnormeOne, gridnormenegOne]\n",
    "          statesignal = torch.tensor([gridAll], dtype=torch.float32).cuda()\n",
    "\n",
    "          action_probs, value = cnn(statesignal)\n",
    "\n",
    "          valid_moves = get_valid_moves(state)\n",
    "\n",
    "          action_probs = action_probs * torch.tensor([valid_moves], dtype=torch.float32).cuda()  # mask invalid moves\n",
    "          action_probs /= torch.sum(action_probs)\n",
    "          action_probs = add_dirichlet_noise(action_probs)\n",
    "          root.expand(state, to_play, action_probs)\n",
    "\n",
    "          for i in range(777):\n",
    "              \n",
    "              node = root\n",
    "              search_path = [node]\n",
    "              print('\\rsimulation:{:.2f}'.format(i),end='')\n",
    "\n",
    "              # SELECT\n",
    "              while node.expanded():\n",
    "                  action, node = node.select_child()\n",
    "                  search_path.append(node)\n",
    "\n",
    "              parent = search_path[-2]\n",
    "              state = parent.state\n",
    "              # Now we're at a leaf node and we would like to expand\n",
    "              # Players always play from their own perspective\n",
    "              next_state, _ = get_next_state(state, player=1, action=action)\n",
    "              # Get the board from the perspective of the other player\n",
    "              next_state = get_canonical_board(next_state, player=-1)\n",
    "\n",
    "              # The value of the new state from the perspective of the other player\n",
    "              value = get_reward_for_player(next_state, player=1)\n",
    "              if value==0 and has_legal_moves(next_state):\n",
    "                  # If the game has not ended:\n",
    "                  # EXPAND\n",
    "                  convert_zero(next_state, gridnormeZero)\n",
    "                  convert_one(next_state, gridnormeOne)\n",
    "                  convert_neg_one(next_state, gridnormenegOne)\n",
    "                  gridAll = [gridnormeZero, gridnormeOne, gridnormenegOne]\n",
    "                  statesignal = torch.tensor([gridAll], dtype=torch.float32).cuda()\n",
    "\n",
    "                  statesignal = statesignal.reshape(3, 6, 7)\n",
    "                  \n",
    "                  action_probs, value = cnn(statesignal)\n",
    "                  valid_moves = get_valid_moves(next_state)\n",
    "                  action_probs = action_probs * torch.tensor([valid_moves], dtype=torch.float32).cuda()  # mask invalid moves\n",
    "                  action_probs /= torch.sum(action_probs)\n",
    "\n",
    "                  node.expand(next_state, parent.to_play * -1, action_probs)\n",
    "\n",
    "              self.backpropagate(search_path, value, parent.to_play * -1)\n",
    "          return root "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0qRVrG70usU"
   },
   "outputs": [],
   "source": [
    "class MCTS_playiter:\n",
    "\n",
    "    def backpropagate(self, search_path, value, to_play):\n",
    "        \"\"\"\n",
    "        At the end of a simulation, we propagate the evaluation all the way up the tree\n",
    "        to the root.\n",
    "        \"\"\"\n",
    "        for node in reversed(search_path):\n",
    "            node.value_sum += value if node.to_play == to_play else -value\n",
    "            node.visit_count += 1\n",
    "\n",
    "    def run(self, state, to_play):\n",
    "      with torch.no_grad():      \n",
    "          gridnormeOne = np.zeros(shape=(6, 7))\n",
    "          gridnormenegOne = np.zeros(shape=(6, 7))\n",
    "          gridnormeZero = np.zeros(shape=(6, 7))\n",
    "          root = Node(0, to_play)\n",
    "          convert_zero(state, gridnormeZero)\n",
    "          convert_one(state, gridnormeOne)\n",
    "          convert_neg_one(state, gridnormenegOne)\n",
    "          gridAll = [gridnormeZero, gridnormeOne, gridnormenegOne]\n",
    "          statesignal = torch.tensor([gridAll], dtype=torch.float32).cuda()\n",
    "\n",
    "          action_probs, value = cnn_iter1(statesignal)\n",
    "\n",
    "          valid_moves = get_valid_moves(state)\n",
    "\n",
    "          action_probs = action_probs * torch.tensor([valid_moves], dtype=torch.float32).cuda()  # mask invalid moves\n",
    "          action_probs /= torch.sum(action_probs)\n",
    "          action_probs = add_dirichlet_noise(action_probs)\n",
    "          root.expand(state, to_play, action_probs)\n",
    "\n",
    "          for i in range(777):\n",
    "              \n",
    "              node = root\n",
    "              search_path = [node]\n",
    "              print('\\rsimulation:{:.2f}'.format(i),end='')\n",
    "\n",
    "              # SELECT\n",
    "              while node.expanded():\n",
    "                  action, node = node.select_child()\n",
    "                  search_path.append(node)\n",
    "\n",
    "              parent = search_path[-2]\n",
    "              state = parent.state\n",
    "              # Now we're at a leaf node and we would like to expand\n",
    "              # Players always play from their own perspective\n",
    "              next_state, _ = get_next_state(state, player=1, action=action)\n",
    "              # Get the board from the perspective of the other player\n",
    "              next_state = get_canonical_board(next_state, player=-1)\n",
    "\n",
    "              # The value of the new state from the perspective of the other player\n",
    "              value = get_reward_for_player(next_state, player=1)\n",
    "              if value==0 and has_legal_moves(next_state):\n",
    "                  # If the game has not ended:\n",
    "                  # EXPAND\n",
    "                  convert_zero(next_state, gridnormeZero)\n",
    "                  convert_one(next_state, gridnormeOne)\n",
    "                  convert_neg_one(next_state, gridnormenegOne)\n",
    "                  gridAll = [gridnormeZero, gridnormeOne, gridnormenegOne]\n",
    "                  statesignal = torch.tensor([gridAll], dtype=torch.float32).cuda()\n",
    "\n",
    "                  statesignal = statesignal.reshape(3, 6, 7)\n",
    "                  \n",
    "                  action_probs, value = cnn_iter1(statesignal)\n",
    "                  valid_moves = get_valid_moves(next_state)\n",
    "                  action_probs = action_probs * torch.tensor([valid_moves], dtype=torch.float32).cuda()  # mask invalid moves\n",
    "                  action_probs /= torch.sum(action_probs)\n",
    "\n",
    "                  node.expand(next_state, parent.to_play * -1, action_probs)\n",
    "\n",
    "              self.backpropagate(search_path, value, parent.to_play * -1)\n",
    "          return root "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "xMWXlVoLsSO6",
    "outputId": "b479ac6a-3440-48ee-d197-c7e737fe22ea"
   },
   "outputs": [],
   "source": [
    "from flask import Flask,request\n",
    "\n",
    "from flask_cors import CORS, cross_origin\n",
    "app = Flask(__name__)  \n",
    "CORS(app)  \n",
    "mctsplay=MCTS_play()\n",
    "mctsplayiter=MCTS_playiter()\n",
    "\n",
    "@app.route(\"/actionblue\", methods=['PUT'])\n",
    "@cross_origin()\n",
    "def actionblue():\n",
    "    global n_step\n",
    "    global reward\n",
    "    global gridnorme\n",
    "    global n_steps\n",
    "    with torch.no_grad():\n",
    "        if request.method == 'PUT':\n",
    "            m = request.data.decode('utf-8')\n",
    "            s = json.loads(m)\n",
    "            if s['winner'] == 0:\n",
    "                gridnorme = np.zeros(shape=(6, 7))\n",
    "                grid = s['gridLine']\n",
    "                for i in range(6):\n",
    "                    for j in range(7):\n",
    "                        if (grid[i][j] == 0):\n",
    "                            gridnorme[i][j] = 0\n",
    "                        elif (grid[i][j] == 2):\n",
    "                            gridnorme[i][j] = 1\n",
    "                        else:\n",
    "                            gridnorme[i][j] = -1\n",
    "                \n",
    "                actions = mctsplay.run(gridnorme, 1)\n",
    "               \n",
    "                actions = torch.tensor([actions.children[visit].visit_count for visit in actions.children],\n",
    "                                       dtype=torch.float32).cuda()\n",
    "                actions /= torch.sum(actions)\n",
    "\n",
    "                colonnes = torch.sort(actions, descending=True)\n",
    "\n",
    "               \n",
    "                action2rotation = [0, 1, 2, 3, 4, 5, 6]\n",
    "               \n",
    "                colonne = action2rotation[colonnes.indices[0]]\n",
    "                j = 0\n",
    "                while colonnepleine(gridnorme, colonne) and sum(\n",
    "                        (x != 0) for x in gridnorme.transpose().flatten()) < 42:\n",
    "                    j = (j + 1) % len(colonnes.indices)\n",
    "                    colonne = action2rotation[colonnes.indices[j]]\n",
    "\n",
    "                colonne = action2rotation[colonne]\n",
    "                lign = 0\n",
    "                while gridnorme[lign][colonne] != 0 and sum(\n",
    "                        (x != 0) for x in gridnorme.transpose().flatten()) < 42:\n",
    "                    lign = (lign + 1) % 6\n",
    "\n",
    "    return str(colonne)\n",
    "\n",
    "@app.route(\"/actionred\", methods=['PUT'])\n",
    "@cross_origin()\n",
    "def actionred():\n",
    "    global n_step\n",
    "    global reward\n",
    "    global gridnorme\n",
    "    global n_steps\n",
    "    with torch.no_grad():\n",
    "        if request.method == 'PUT':\n",
    "            m = request.data.decode('utf-8')\n",
    "            s = json.loads(m)\n",
    "            if s['winner'] == 0:\n",
    "                gridnorme = np.zeros(shape=(6, 7))\n",
    "                grid = s['gridLine']\n",
    "                for i in range(6):\n",
    "                    for j in range(7):\n",
    "                        if (grid[i][j] == 0):\n",
    "                            gridnorme[i][j] = 0\n",
    "                        elif (grid[i][j] == 2):\n",
    "                            gridnorme[i][j] = 1\n",
    "                        else:\n",
    "                            gridnorme[i][j] = -1\n",
    "                \n",
    "                actions = mctsplayiter.run(gridnorme, 1)\n",
    "               \n",
    "                actions = torch.tensor([actions.children[visit].visit_count for visit in actions.children],\n",
    "                                       dtype=torch.float32).cuda()\n",
    "                actions /= torch.sum(actions)\n",
    "\n",
    "                colonnes = torch.sort(actions, descending=True)\n",
    "\n",
    "               \n",
    "                action2rotation = [0, 1, 2, 3, 4, 5, 6]\n",
    "               \n",
    "                colonne = action2rotation[colonnes.indices[0]]\n",
    "                j = 0\n",
    "                while colonnepleine(gridnorme, colonne) and sum(\n",
    "                        (x != 0) for x in gridnorme.transpose().flatten()) < 42:\n",
    "                    j = (j + 1) % len(colonnes.indices)\n",
    "                    colonne = action2rotation[colonnes.indices[j]]\n",
    "\n",
    "                colonne = action2rotation[colonne]\n",
    "                lign = 0\n",
    "                while gridnorme[lign][colonne] != 0 and sum(\n",
    "                        (x != 0) for x in gridnorme.transpose().flatten()) < 42:\n",
    "                    lign = (lign + 1) % 6\n",
    "\n",
    "    return str(colonne)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='10.142.128.60', port=5002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Connect4_alpha_zero_ngrok.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "pytorch-gpu.1-8.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m65"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
